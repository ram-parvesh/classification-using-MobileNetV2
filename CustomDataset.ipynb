{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "684c2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0055e2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ram/classification-using-MobileNetV2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abe77a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ef3f6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 988, 1660])\n",
      "torch.Size([3, 231, 424])\n",
      "torch.Size([3, 398, 654])\n",
      "torch.Size([3, 115, 164])\n",
      "torch.Size([3, 107, 134])\n",
      "torch.Size([3, 57, 81])\n",
      "torch.Size([3, 350, 592])\n",
      "torch.Size([3, 357, 399])\n",
      "torch.Size([3, 988, 1660])\n",
      "torch.Size([3, 231, 424])\n"
     ]
    }
   ],
   "source": [
    "annotations_file='test.csv'\n",
    "train =pd.read_csv(annotations_file)\n",
    "train=train.iloc[:,0]\n",
    "img_dir='images'\n",
    "for idx ,i in enumerate(train):\n",
    "    if idx<10:\n",
    "        img_path = os.path.join(img_dir,i)\n",
    "        images = read_image(img_path)\n",
    "        print(images.shape)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "159d383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "torch.Size([3, 96, 182]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 235, 325]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 595, 1176]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 254, 404]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 394, 546]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 785, 561]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 619, 477]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 162, 266]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 276, 410]) 0\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 192, 332]) 0\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomDatasetFromImages():\n",
    "    def __init__(self,img_dir, csv_path,transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            img_path (string): path to the folder where images are\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        #image Dir\n",
    "        self.img_dir = img_dir\n",
    "        # Read the csv file\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # First column contains the image paths and image name\n",
    "        self.image_arr = self.df.iloc[:, 0]\n",
    "        # forth column is for an label\n",
    "        self.label = self.df.iloc[:, 3]\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.image_arr[idx])\n",
    "        images = read_image(img_path)\n",
    "        label = self.label[idx]\n",
    "        if self.transforms is not None:\n",
    "            images = self.transforms(images)\n",
    "\n",
    "        return images , label\n",
    "            \n",
    "    def visualize(self):\n",
    "        print(self.img_dir)\n",
    "        \n",
    "        for idx in range(len(self.label)):\n",
    "            if idx <10:\n",
    "                # print(\"images path:\",self.image_arr)\n",
    "                # print('label :',self.label)\n",
    "                img_path = os.path.join(self.img_dir, self.image_arr[idx])\n",
    "                images = read_image(img_path)\n",
    "                label = self.label[idx]\n",
    "\n",
    "                print(images.shape,label)\n",
    "                print(type(images))\n",
    "                # print(images.shape)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Call dataset\n",
    "    img_dir ='images'\n",
    "    annotations_file = 'train.csv'\n",
    "    customfrom_images = CustomDatasetFromImages(img_dir,annotations_file)\n",
    "    customfrom_images.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fb81eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "img_dir ='images'\n",
    "annotations_file = \"train.csv\"\n",
    "input_size = 224\n",
    "\n",
    "\n",
    "\n",
    "transforms = transforms.Compose([transforms.ToPILImage(),transforms.Resize([int(224), int(224)]),transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = CustomDatasetFromImages(img_dir,annotations_file,transforms=transforms )\n",
    "img_dir ='images'\n",
    "annotations_file = \"val.csv\"\n",
    "\n",
    "val_dataset = CustomDatasetFromImages(img_dir,annotations_file ,transforms=transforms)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset ,batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e128a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224]) tensor([65, 47, 19, 66, 14])\n",
      "torch.Size([5, 3, 224, 224]) tensor([37, 15, 80, 77, 26])\n",
      "torch.Size([5, 3, 224, 224]) tensor([ 9,  7, 40, 22, 80])\n",
      "torch.Size([5, 3, 224, 224]) tensor([81, 54, 80, 42, 11])\n",
      "torch.Size([5, 3, 224, 224]) tensor([77, 68, 48, 37, 50])\n",
      "torch.Size([5, 3, 224, 224]) tensor([55, 75, 56, 38, 28])\n",
      "torch.Size([5, 3, 224, 224]) tensor([63, 78, 75,  4,  3])\n",
      "torch.Size([5, 3, 224, 224]) tensor([21, 32, 67, 30, 12])\n",
      "torch.Size([5, 3, 224, 224]) tensor([ 3, 46, 75, 18, 13])\n",
      "torch.Size([5, 3, 224, 224]) tensor([59, 23,  3, 72, 15])\n"
     ]
    }
   ],
   "source": [
    "for i,(images,labels) in enumerate(train_loader):\n",
    "    if i<10:\n",
    "        print(images.shape ,labels )\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b479870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "def dwise_conv(ch_in, stride=1):\n",
    "    return (\n",
    "        nn.Sequential(\n",
    "            #depthwise\n",
    "            nn.Conv2d(ch_in, ch_in, kernel_size=3, padding=1, stride=stride, groups=ch_in, bias=False),\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "    )\n",
    "\n",
    "def conv1x1(ch_in, ch_out):\n",
    "    return (\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=1, padding=0, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def conv3x3(ch_in, ch_out, stride):\n",
    "    return (\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "class InvertedBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, expand_ratio, stride):\n",
    "        super(InvertedBlock, self).__init__()\n",
    "\n",
    "        self.stride = stride\n",
    "        assert stride in [1,2]\n",
    "\n",
    "        hidden_dim = ch_in * expand_ratio\n",
    "\n",
    "        self.use_res_connect = self.stride==1 and ch_in==ch_out\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(conv1x1(ch_in, hidden_dim))\n",
    "        layers.extend([\n",
    "            #dw\n",
    "            dwise_conv(hidden_dim, stride=stride),\n",
    "            #pw\n",
    "            conv1x1(hidden_dim, ch_out)\n",
    "        ])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.layers(x)\n",
    "        else:\n",
    "            return self.layers(x)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, ch_in=3, n_classes=1000):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "\n",
    "        self.configs=[\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1]\n",
    "        ]\n",
    "\n",
    "        self.stem_conv = conv3x3(ch_in, 32, stride=2)\n",
    "\n",
    "        layers = []\n",
    "        input_channel = 32\n",
    "        for t, c, n, s in self.configs:\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                layers.append(InvertedBlock(ch_in=input_channel, ch_out=c, expand_ratio=t, stride=stride))\n",
    "                input_channel = c\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.last_conv = conv1x1(input_channel, 1280)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Linear(1280, n_classes)\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.last_conv(x)\n",
    "        x = self.avg_pool(x).view(-1, 1280)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# if __name__==\"__main__\":\n",
    "#     # model check\n",
    "#     model = MobileNetV2(ch_in=3, n_classes=1000)\n",
    "#     summary(model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7735444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(ch_in=3, n_classes=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45279802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing images with specific size using python\n",
    "import os\n",
    "from PIL import Image\n",
    "folder_images = \"dat\"\n",
    "for filenames in os.walk(folder_images):\n",
    "    img = Image.open(filenames)\n",
    "    h, w = img.shape\n",
    "    if(not (h >= 160 or w >= 160)):\n",
    "        os.remove(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e38dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddacf3edb7c4b001620934eb4de048fb99cf4741745d5e51453b48381cacc602"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
